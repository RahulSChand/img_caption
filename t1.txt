torch.Size([1, 590, 768])
kwargs_has_attention_mask:  False
shape: torch.Size([1, 590, 768])
model_kwargs torch.Size([1, 590])
is encoder decoder
torch.Size([])
tensor(2, device='cuda:0')
inputs_embeds dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs'])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[2]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[0]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[1121]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[42]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[2274]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[52]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[64]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[192]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[10]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[3627]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[15]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[5]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[514]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[4]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[345]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[32]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[171]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[82]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[11]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[5]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[3627]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[4]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[345]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[16]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[10]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[38780]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[11]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[5]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[514]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[23]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[5]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[2576]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[9]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[5]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[2274]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
model_kwargs dict_keys(['inputs_embeds', 'use_cache', 'attention_mask', 'encoder_outputs', 'cache_position', 'past_key_values'])
use cache torch.Size([1, 590])
decoder_input_ids: tensor([[4]], device='cuda:0')
encoder_outputs[0] torch.Size([1, 590, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
hidden_states: torch.Size([1, 1, 768])
